## Consistency Optimization
Consistency-optimized Test-Time Adaptation (TTA) algorithms typically maintain source-free online prediction consistency within a joint optimization framework over input perturbation and model parameter spaces. This is achieved by enforcing a self-supervised perturbation invariance constraint on features and predictions, such as the Mean-Teacher model, which utilizes an Exponential Moving Average (EMA) for implicit distillation.
- `CoTTA`[CVPR'2022]**Continual Test-Time Domain Adaptation**[[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Continual_Test-Time_Domain_Adaptation_CVPR_2022_paper.pdf)][[G-Scholar](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Continual+Test-Time+Domain+Adaptation&btnG=)][[code](https://qin.ee/cotta)]   
CoTTA optimizes consistency in both temporal and spatial dimensions. It uses soft pseudo-labels from an EMA teacher model as the objective, encouraging consistency between teacher and student predictions. CoTTA also introduces an adaptive sample augmentation mechanism to handle domain gaps, leveraging the source model's prediction probability as confidence. If confidence is below a threshold, the sample is augmented multiple times, and the teacher model's average prediction on these augmented samples serves as the final pseudo-label.   
Edge-friendly&nbsp;ðŸ”¶   
CoTTA is well-suited for streaming online scenarios, which offers advantages in edge deployment settings such as autonomous driving. However, its full parameter update and the dual forward computation of teacher-student models may present challenges in scenarios with extremely limited computational resources.


