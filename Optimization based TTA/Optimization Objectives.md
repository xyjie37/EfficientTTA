## Optimization Objectives
Methods that optimize based on unsupervised objectives constitute a classical category of Test-Time Adaptation(TTA) algorithms. These approaches are implemented through minimizing predictive uncertainty, adapting batch statistics, or leveraging density estimation, and are characterized by their reliance on label-free objective functions derived from model outputs or data distributions to align the model with the target domain without supervised signals. In subsequent discussions, we will systematically introduce unsupervised optimization-based Test-Time Adaptation(TTA) algorithms according to their different methodological approaches and analyze whether they are edge-friendly.
### entropy minimization
The essence of entropy-minimization Test-Time Adaptation (TTA) is to take information uncertainty as the optimization signal and realize model self-calibration under the test data distribution through source-free domain online optimization.  
- `Tent`[ICLR'2021]**Tent: Fully Test-time Adaptation by Entropy Minimization**[[paper](https://arxiv.org/abs/2006.10726)][[code](https://github.com/DequanWang/tent)]  
Tent is a Test-Time Adaptation(TTA) method that operates exclusively on the model and the current unlabeled test data. Its core principle is to minimize the Shannon entropy of the modelâ€™s predictions. This process dynamically tunes the affine parameters of the normalization layers, which in turn improves the modelâ€™s generalization performance under distribution shift.  
Edge-friendly&nbsp;âœ…   
Tent achieves its update in merely one forward and one backward pass, restricting optimization to the affine parameters of the normalization layers.
- `DSI`[CoRR'2019]**Dynamic Scale Inference by Entropy Minimization**[[paper](https://arxiv.org/pdf/1908.03182)]   
DSI is a Test-Time Adaptation(TTA) algorithm designed to optimize the trade-off between accuracy and computational efficiency by dynamically selecting the modelâ€™s input scale. It employs prediction entropy as a confidence measure. Starting from the lowest resolution, the method incrementally scales the input. At each scale, the classification entropy is computed; if it drops below a set threshold, the current prediction is output and the inference is halted, thus avoiding the computational cost of processing higher resolutions.   
Edge-friendly&nbsp;âœ…   
DSI introduces a multi-resolution inference mechanism and an entropy-based early stopping mechanism. On the ImageNet dataset, it achieves a reduction of approximately 30% in average FLOPs at the cost of only a 0.1% drop in accuracy.
- `EATA`[ICML'2022]**Efficient Test-Time Model Adaptation without Forgetting**[[paper](https://proceedings.mlr.press/v162/niu22a.html)][[code](https://github.com/mr-eggplant/EATA)]   
EATA is a test-time adaptation method that selectively updates a refined sample subset. It employs a dual-filtering mechanism, based on entropy and diversity weights, to exclude unreliable and redundant samples. On this filtered subset, it performs entropy minimization via backpropagation, while a Fisher regularization term is concurrently applied to mitigate forgetting of in-distribution data during continual adaptation.  
Edge-friendly&nbsp;âœ…  
By selectively backpropagating through a filtered subset of samples and exclusively updating the affine parameters in its BN layers, EATA reduces inference time by 10% relative to Tent on ImageNet-C.
- `MEMO`[NeurIPS'2022]**MEMO: Test Time Robustness via Adaptation and Augmentation**[[paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/fc28053a08f59fccb48b11f2e31e81c7-Paper-Conference.pdf)][[code](https://github.com/zhangmarvin/memo)]
MEMO is a Test-Time Adaptation(TTA) algorithm that minimizes the entropy of the marginal predictive distribution over multiple augmented views. The algorithm generates multiple augmented samples from an input, computes their marginal predictive distribution, and optimizes the model parameters to minimize the entropy of this distribution. It then uses the updated model to predict on the original input sample, thereby reducing the impact of non-semantic perturbations.   
Edge-friendly&nbsp;ðŸ”¶   
MEMO enables single-sample adaptation, making it suitable for edge-based streaming scenarios. However, its update of all model parameters, compared to updating only the Batch Normalization (BN) affine parameters, incurs higher computational costs.
- `SAR`[ICLR'2023] **TOWARDS STABLE TEST-TIME ADAPTATION INDYNAMIC WILD WORLD**[[paper](https://openreview.net/pdf?id=g2YraF75Tj)][[code](https://github.com/mr-eggplant/SAR)]
SAR is a Test-Time Adaptation(TTA) algorithm that integrates Reliable Sample Filtering, Sharpness-Aware Minimization, and a Model Recovery Scheme. The algorithm performs model updates exclusively on low-entropy samples and minimizes the worst-case entropy within a neighborhood to avoid converging to sharp minima. To prevent model collapse due to accumulated erroneous updates, SAR maintains a moving average of entropy and rolls back the model when this metric drops anomalously low.   
Edge-friendly&nbsp;ðŸ”¶   
SAR does not address optimizations for computational efficiency. Its support for online updates and the characteristic of updating only affine transformation parameters make it suitable for edge deployment. However, its Sharpness-Aware Minimization introduces greater computational overhead compared to algorithms such as Tent.


